---
title: "Retail Marketing Analytics Assignment"
subtitle: "Evaluating the impact of hired salespeople on espresso machine sales"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    theme: paper
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(dplyr) 
library(tidyverse) 
library(ggplot2)
library(openxlsx)
library(lme4)
library(janitor)
library(lattice)
library(readxl)
library(stargazer)
library(Mcomp)
library(smooth)
library(greybox)
library(modelr)
```

## Executive Summary 

As part of a consultation project that I undertook, my goal was to provide actionable insights through data analysis for my assigned client; a consumer electronics brand that features their products in mass retail stores. The marketing manager was interested in whether hiring salespeople to promote their products within those stores was effective. I achieved this by building a SCAN\*PRO model in R to quantify the incremental benefit of this marketing strategy. Since the SCAN\*PRO model is evaluated at store level, and our client had placed salespeople in more than 100 stores, a multilevel adaptation of the SCAN\*PRO model was used to account for the differences between stores. The data set that was used focuses on the sales of espresso machines. The model has confirmed that there is a positive correlation between the number of days that a salesperson is present and the average weekly sales per store. At the end of the analysis, the incremental sales and profitability of the salesperson scheme was evaluated and discussed.

Due to a Non-Disclosure Agreement (NDA) that has been signed with our client, the data sets used will not be shared, and any sensitive information has been amended to safeguard the confidentiality of the company. For this reason, throughout this assignment the company will be referred to as Company X. 

## 1. Introduction 

As part of a promotional campaign, Company X has been hiring trained salespeople who are placed in the company's allocated area inside mas retail stores. Their role is to engage with customers to promote the company's products so as to boost sales.

Now more than ever, consumers rely heavily on online information to make purchasing decisions. It could thus be argued that hiring salespeople puts a company at a competitive disadvantage, and marketing budget could be spent on more impactful strategies. 

The main focus of this assignment will be to assess the effectiveness of hiring salespeople using a SCAN\*PRO model. Since the SCAN\*PRO model is evaluated for each product separately, this assignment will only focus on espresso machine sales. The particular product was chosen it is a perfect example of a domestic appliance which consumers usually research online before they buy. Therefore, the impact of a salesperson is sebatable. 

In addition, the SCAN\*PRO model should be evaluated at a store-level. This is because different stores are in different locations, serve different customer demographics and have different sizes, leading to varying levels of promotion impacts. This would require the addition of numerous variables in the model. However, since the data is hierarchical (with weekly sales nested within different stores), a multilevel approach will be used in order to account for different store effects.

The key question that will be answered by the end of this assignment is: Does the presence of salespeople positively affect weekly espresso machine sales or do consumers rely more on information that they find online? 

## 2. Data and Model

### 2.1 Dataset 

Two data sets were sourced and cleaned as shown in Appendix A. The first data set provided individual sales data while the other recorded the daily presence of salespeople. The data was aggregated on a weekly level to capture potential lag effects. For example, a customer could have interacted with a salesperson on Monday, and decided to come back and make a purchase during the weekend, when the salesperson was no longer present. 

Since the past year has been particularly unusual due to social-distancing restrictions, only 2018 and 2019 data will be used. An extract of the final data frame is shown below:

```{r include=FALSE}
# Code to import clean data set  
setwd('~/Desktop/RMA/Individual')
df <- read_xlsx('Clean_Data.xlsx')
df <- data.frame(df)
```

```{r echo=FALSE}
kable(df[30:35,])
```

It is important to note that the brand offers a series of different espresso machine models that have varying prices. As the original data set did not contain product codes, the data was restricted to a price range of 200-400 euros and the average was calculated for each store for each week. 

The following plot shows the aggregate weekly sales across all stores throughout the two years. 

<div align="center">
```{r echo=FALSE, message=FALSE, warning=FALSE}
df_agg <- df %>% select(2,3) %>% group_by(Week) %>% summarize(Units = sum(Units_Sold))
df_agg %>% ggplot(aes(x=Week, y =Units)) + geom_line(col='blue') + theme_bw() + ggtitle('Total Weekly Sales')
```
</div>

The large spikes on Weeks 47 and 100 correspond to Black Friday sales. We can also see a very slight seasonality with higher demand over the Christmas period, and lower demand over the summer. Spikes are also evident in the first week of the year. 

In order to assess the need for a Multilevel Model, the following plot was created by fitting a simple null multilevel model (no predictors). 

<div align="center">
```{r echo=FALSE}
# Null multilevel model
null <- lmer(Units_Sold ~ (1 | Store), data = df) 
# Random Effects plot
res <- ranef(null, condVar = TRUE) # calculating residuals from random effects of model
store <- as.factor(rownames(res[[1]])) # extracting store names
table <- cbind(store, res[[1]]) # adding stores and residuals in one table
colnames(table) <- c("store","residuals")
table <- table[order(table$residuals), ] # order by residuals
table <- cbind(table, c(1:dim(table)[1]))
colnames(table)[3] <- "rank" # add rank
table <- table[order(table$store), ] # order 
plot(table$rank, table$residuals, type = "n", xlab = "stores (ranked)", ylab = "Residuals")

points(table$rank, table$residuals, col = "blue")
abline(h = 0, col = "red")
title('Store deviation from Units_Sold mean')
```
</div>

The residuals represent each store’s departure from the overall mean. The stores below the red line had below-average sales while the rest had above-average sales. It is clear that the range of mean sales is very widespread which is expected. Therefore, a multilevel model could be suitable. 

### 2.2 SCAN*PRO Model Specification 

To assess the effectiveness of salespeople presence, a SCAN\*PRO model was considered as it enables us to capture the incremental benefit. Normally, the model can capture the effect of price discounts by dividing the current price by the median regular unit price. Since we don't have any price discount data, the weekly average price for each store is added in the model. 

In addition, the model specification suggests that a set of dummy variables should be added to indicate whether the observation is in week t. This would lead to our model having 104 dummy variables, thus increasing model complexity. Extracting the seasonality of weekly data has also proven to be a challenge, since weekly variations are not always periodic. For example, Black Friday does not fall on the same week of the year. Therefore, we cannot seasonally difference the time series. 

As a result, this method has been criticized by econometricians such as Ross Link, who suggests that those dummy variables sometimes 'accidentally' pick up effects that negatively impact the accuracy of the model coefficients$^1$. . Instead, he recommends using a smoothed category base volume as a seasonality indicator$^2$, and then adding specific dummies for public holidays and events like Black Friday. 

Finally, since the SCAN\*PRO model is evaluated at store level, it would require us to create and add more than 100 dummy variables to capture the store effects. Instead of doing that, the SCAN\*PRO model will firstly be evaluated for just 1 store, and then a multilevel adaptation will be considered. The SCAN\*PRO formula that will be used is as follows: 


$$Sales_t = \beta \times (Price)^{OWNELAS} \times (DaysEffect)^{DAYS} \times (SeasonalEffect)^{INDEX} \\
\times (XmasEffect)^{XMAS} \times (BfEffect)^{BF} \times (FwEffect)^{FW} \times e^{\epsilon_t}$$

In the model equation above, $\beta$ stands for the model intercept and $e_t$ is the regression error for week t. This corresponds to the external factors which we cannot account for in our model. The weekly average price is raised to the power $OWNELAS$ which represents the own elasticity and is to be estimated. To model the Days, Seasonal, Christmas, Black Friday and first week of the year effects, we raise those unknown variables to the power of $DAYS$, $INDEX$, $XMAS$, $BF$ and $FW$, which correspond to the number of days a salesperson was present in a week, the seasonal index calculated earlier, and three binary variables indicating whether the observation lies in Christmas, Black Friday, or first week of the year. Since this is a multiplicative model, it requires optimization techniques in order to minimize the sum of squared errors. As an alternative, we can take the log of both sides to convert it into a linear regression model as shown below: 

$$Sales_t = \beta' + (OWNELAS)Log(Price) + Log(DaysEffect)(DAYS) + Log(SeasonalEffect)(INDEX) + \\
Log(XmasEffect)(XMAS) + Log(BfEffect)(BF) + Log(FwEffect)(FW) + \epsilon_t$$

Since the $Days$ variable represents the total number of days that a salesperson was present in a week, it contains many 0 entries. Therefore, it is not mathematically wise to take its log. This is why in the above formula, we will be estimating $DaysEffect$ instead of $DAYS$, and will thus treat $Log(DaysEffect)$ as the coefficient. The actual scale of the coefficient will then be calculated by taking the exponential of the estimated value obtained. The similar thinking goes for the Seasonal, Christmas, Black Friday and first week effects. 

### 2.3 Multilevel Model Specification 

Any data set that has individual observations nested within higher-levels is called hierarchical and can be modeled using Multilevel Modeling. In our case weekly sales are grouped within Stores, thus allowing us to transform the original SCAN\*PRO into a multilevel model. For the purpose of this assignment, two models will be considered: A random intercept, and random intercept/slope model. 

The random intercept model allows us to vary the intercept of each regression line across different stores. In this case, the slope stays the same for all regression lines, meaning that the relationship between the dependent and independent variables does not change. This enables us to account for the different levels of weekly sales that arise within each store (eg. Larger stores usually have more sales). The random intercept and slope model on the other hand, allows us to vary the relationship between the predictor and dependent variable for each store. This is equivalent to saying that the presence of a salesperson has a different impact on sales for each store. The formulation of both models is very similar to the single level SCAN\*PRO with a small alteration that allows for the variation of intercepts and slopes: 

**Random Intercept:**

$$Sales_{t,j} = \beta + (OWNELAS)Log(Price)_{t,j} + Log(DaysEffect)(DAYS)_{t,j} + Log(SeasonalEffect)(INDEX)_{t,j} + \\
Log(XmasEffect)(XMAS)_{t,j} + Log(BfEffect)(BF)_{t,j} + Log(FwEffect)(FW)_{t,j} + u_{0j}$$

**Random Intercept and Slope:**

$$Sales_{t,j} = \beta + (OWNELAS)Log(Price)_{t,j} + Log(DaysEffect)(DAYS)_{t,j} + Log(SeasonalEffect)(INDEX)_{t,j} + \\
Log(XmasEffect)(XMAS)_{t,j} + Log(BfEffect)(BF)_{t,j} + Log(FwEffect)(FW)_{t,j} + u_{0j} + u_{1j}(DAYS)_{t,j} + \epsilon_{t,j}$$


The above models present the estimated sales on week t for store j. In both cases, the intercept of each store's regression line is given by $\beta + u_{0j}$. For the random slope model, the slope of each store is given by $Log(DaysEffect)+u_{1j}$.  


## 3. Results 

Before building the models, the aggregate weekly sales across all stores were exponentially smoothed using a smoothing parameter $\alpha = 0.1$ in order to create a seasonal index. The lower the smoothing parameter the less responsive the model is to shocks. Therefore, in order to smooth out spikes such as Black Friday sales, the smoothing parameter was set to be very small. The exponentially smoothed values are shown in red in the plot below. 

<div align="center">
```{r echo=FALSE}
exp_smoothing <- ses(df_agg$Units, alpha=0.1, initial="simple")

plot(df_agg$Units, type = 'l', ylab='Units Sold', xlab='Week')
lines(exp_smoothing$fitted, col='red')
title('Exponentially Smoothed Weekly Sales across all Stores')
```
</div>


```{r include=FALSE}
# Adding smoothed category baseline volume to data frame
fval <- data.frame(Week = seq(1:105), Index = exp_smoothing$fitted)
df <- left_join(df, fval, by = 'Week')
```

```{r include=FALSE}
# Adding Dummy Variables 
df <- df %>% mutate(XMAS = ifelse(Week == 52 | Week == 104, 1, 0))
df <- df %>% mutate(BF = ifelse(Week == 47 | Week == 100, 1, 0))
df <- df %>% mutate(FW = ifelse(Week == 1  | Week == 105, 1, 0))
```

```{r include=FALSE}
# Creating columns for the log values 
df$Units_Sold_log <- log(df$Units_Sold)
df$Avg_Price_log <- log(df$Avg_Price)
```

### 3.1 Single-Level SCAN*PRO 

```{r eval=FALSE, include=FALSE}
# Code to identify stores with the highest number of observations 
table(df$Store)[table(df$Store) == max(table(df$Store))]
```

Firstly, a single level SCAN*PRO model was ran for the store that had the highest number of entries (ie. Stores 48). The formula used is the same as the one introduced in section 2.2. The output is shown below:

```{r include=FALSE}
# Filtering dataset to only include data for store 48
store_48 <- df %>% filter(Store == 'Store 48')
```

```{r include=FALSE}
# Creating Models 
model_48 <- lm(Units_Sold_log ~ Avg_Price_log + Days + Index + XMAS + BF + FW, data = store_48)
```

<div align="center">
```{r echo=FALSE, results="asis"}
stargazer(model_48, type = 'html', title = 'Regression Results')
```
</div>

*** 

First of all we can see that the number of days that a salesperson was present has a statistically significant effect on weekly sales (p-value less than 0.05). In particular, an additional day per week can increase the number of espresso machines sold in store 48 by 1.8%. The Seasonal, as well as Black Friday effects are also classified as statistically significant. As expected, Black Friday week increases sales by 17%. 

Looking at the coefficient estimates for the $Log(AvgPrice)$ variable, the negative value suggests that a higher price will lead to lower sales by 2.6%. This is expected, since an increase in price will naturally reduce sales and vice versa. However, it must be noted that it is not statistically significant, which could be due to the fact that we're using average prices across different espresso machine models.

Finally, the R-squared value of 0.48 suggests that 48% of the variation in the data has been successfully explained by the model. 

Using the estimated coefficients in the table above, we obtain the following model equation: 

$$Sales = 12.471 \times (Price)^{-0.257} \times (DaysEffect)^{0.176} \times (SeasonalEffect)^{ 0.001} \times (XmasEffect)^{0.004} \times (BfEffect)^{1.708} \times (FwEffect)^{0.009}$$ 

### 3.2 Multilevel SCAN*PRO Model

A random intercept(model1) and random intercept/slope (model2) variation of the SCAN*PRO model will now be fitted to account for store differences. The regression output is presented in the table below. Due to a warning message that appears suggesting that some predictor variables are on very different scales, the seasonal index is normalized by dividing all entries by the maximum value. 

<div align="center">
```{r echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# Adding normalized column 
df <- df %>% mutate(Index_norm = Index/max(Index))

# Model
model1 <- lmer(Units_Sold_log ~ Avg_Price_log + Days + Index_norm + XMAS + BF + FW + (1 | Store), data=df)
model2 <- lmer(Units_Sold_log ~ Avg_Price_log + Days + Index_norm + XMAS + BF + FW + (Days| Store), data=df)

stargazer(model1, model2, type = 'html', title = 'Regression Results', keep.stat="n")
```
</div>

*** 

The Days, Seasonal Index, Black Friday and First Week variables prove to be significant in both models. In fact the coefficient estimates are very similar for each model. Focusing on model 1, it suggests that after accounting for price and seasonality factors, one extra day per week leads to an average weekly sales increase of 1.6% across all stores. In addition, the r-squared values of each model are shown below: 

```{r echo=FALSE}
print(paste('Model 1 =', round(rsquare(model1, df),2)))
print(paste('Model 2 =', round(rsquare(model2, df),2)))
```

In order to understand whether the variation of the slope was needed, a Likelihood Ratio Test was carried out. The Null hypothesis of the test is that there is no difference on the impact of days per week across different stores. The formula of the test is shown below and the resulting value is compared to the 5% chi-squared distribution value with degrees of freedom equal to the additional parameters that have been added to the model (in this case 8, since the days range from 0 to 7). 

$$LR = 2*(logL1 - logL2)$$
The results obtained are: 

```{r echo=FALSE}
 
print(paste('Likelihood Ratio Test Value = ', round(-2*(logLik(model1)-logLik(model2)),2)))

print(paste('5% level of Chi Squared Distribution on 8 Degrees of Freedom =', round(qchisq(.95, df=8),2)))
```

Since the test result is less than the critical value of 15.51, we fail to reject the null hypothesis and conclude that there are no significant differences of the impact of salesperson presence on sales across different stores. Therefore, the random intercept model will be used for the final calculation. 

If the null hypothesis had been rejected, a plot of intercepts against slopes could be used in order to differentiate which stores achieve a higher impact by hiring salespeople. This would in turn allow us to propose a suitable budget allocation for the number of days that each store should get salespeople. A brief analysis based on the current data is presented in Appendix B.


### 3.3 Incremental Sales and Profitability 

Now that the final model is ready, the incremental sales that can be attributed to the inclusion of salespeople can be calculated for the two year period. Firstly, the baseline sales are calculated for the past two years. This value is estimated by running the same model without the days predictor.  

```{r echo=FALSE}
model_base <- lmer(Units_Sold_log ~ Avg_Price_log + Index_norm + XMAS + BF + FW + (1 | Store), data=df)
baseline_sales = round(fitted(model_base),0)
baseline = sum(baseline_sales)
print(paste('Baseline Sales =', baseline))
```

The incremental sales are then calculated by subtracting the baseline value from the actual units sold throughout the two years: 

```{r echo=FALSE}
print(paste('Incremental Sales = Actual Sales - Baseline Sales = ', sum(df$Units_Sold), '-', round(baseline,2), '=', round(sum(df$Units_Sold) - baseline,2)))
```

According to the above calculations, the sales of 45k units over 2018-2019 can be attributed to the presence of salespeople. However, a key limitation is that the model only managed to fit ~43% of the data, thus making this result questionable. 

In order to estimate the profitability of this marketing program, the incremental revenue was calculated by using the weekly average prices per store, and subtracting the approximate cost of having a salesperson present for one day, which is 255 euros. 

```{r echo=FALSE}
prof <- data.frame(Incremental_Sales = df$Units_Sold - baseline_sales, Avg_Price = df$Avg_Price, Days = df$Days)

prof <- prof %>% mutate(Costs = Days*255, Incremental_Revenue = Incremental_Sales*Avg_Price)

print(paste('Profitability = ', round(sum(prof$Incremental_Revenue) - sum(prof$Costs),2)))

```

Using the above formula, the profitability is estimated to be approximately 12 million euros. 

## 4. Discussion

Overall, it is clear that investing on salespeople is a beneficial strategy for the company. However the extent to which the figures obtained are accurate is debatable. This is because there are numerous assumptions that had to be taken, and some external factors which we could not take into account. 

First of all, the final model could be further improved if we had competitor price data. This would also allow us to calculate the cross price elasticity between our client and different competitors. A variable indicating whether a competitor’s salesperson was present in the store could also be something worth investigating. 

In addition, as the data set did not contain product codes, it was not possible to calculate a weighted price average across different espresso machines. It might therefore be the case, that the own-brand elasticity is not accurate. This is indeed the case in the output table of the Multilevel models, where the coefficient estimate for the average price is positive. This is counter intuitive since it suggests that an increase in price will lead to more sales. 

Another limitation concerns the available sample size for each store. The number of observations varied a lot across stores which could in turn negatively impact the estimation of the multilevel model. 

Finally, an important assumption that was made is that only one salesperson is present in store per day. It could actually be the case that a higher number of salespeople is present at the same time, which could have a more or less positive impact. 

To extend this analysis, different models could be consider by testing additional predictor variables. Moreover, a further study could look into how this in-store experience affects customers in the long run, by using a Vector Autoregressive Regression (VAR) model. This would allow us to capture the long-term effects of customer-salesperson encounters. 

## References 

1. Strahl, Jonas. “Efficiency Maximization of Retail Campaigns.” Bachelor’s Thesis, sal.aalto.fi/publications/pdf-files/tstr11_public.pdf. 
2. Hanssens, Dominique M., et al. Market Response Models: Econometric and Time Series Analysis. Kluwer Academic Publishers, 2004. 

## Appendix 

### Appendix A (Data Wrangling) 

```{r eval=FALSE, include=TRUE}
# Importing Data 
setwd('~/Desktop/RMA/Individual')
data <- read.xlsx('data.xlsx')
```
  
```{r eval=FALSE, include=TRUE}
# Getting rid of NULL and negative (returns) values in Sell.Out.Value column
data_new <- data %>% filter(!is.null(Sell.Out.Value) & Sell.Out.Value != 'NULL' & Sell.Out.Value > 0)

# Selecting columns we need
data_new <- data_new %>% select(1,2,3,5,6)
```

```{r eval=FALSE, include=TRUE}
# Changing Column Names 
names(data_new) <- c('Store', 'Date', 'Quantity', 'Category', 'Value')
```

```{r eval=FALSE, include=TRUE}
# Formatting Date Column 
data_new$Date <- as.Date(data_new$Date , format = "%d.%m.%Y")

# Adding 'Year' Column 
data_new$Year <- format(data_new$Date, format = "%Y")

# Filtering out 2020 because unpredictable year 
data_new <- data_new %>% filter(Year %in% c(2018, 2019))

```

```{r eval=FALSE, include=TRUE}
# Formatting Value column as numeric 
data_new$Value <- as.numeric(sub(",", ".", data_new$Value, fixed = TRUE))

# Formatting Quantity column as numeric 
data_new$Quantity <- as.numeric(data_new$Quantity)
```

```{r eval=FALSE, include=TRUE}
# Filter data to only contain Domestic Appliances

espresso <- data_new %>% filter(Category == '0418 Espresso')

# Removing number code from category names 
espresso$Category <- str_sub(espresso$Category, 6)

# Getting unit price
espresso <- espresso %>% mutate(Unit_price = Value/Quantity)

# Filtering price range to represent an espresso machine model of the most popular price range
espresso <- espresso %>% filter(between(Unit_price, 200, 400))
```

```{r eval=FALSE, include=TRUE}
# We will aggregate data per week
espresso$Week <-as.numeric(strftime(espresso$Date, format = '%V'))
espresso$Month <-as.numeric(strftime(espresso$Date, format = '%m'))

# Adding this here because otherwise the end of a year is labelled as the first week of the year
espresso<- espresso %>% mutate(Year_Adj = ifelse(Week ==1 & Month == 12, as.numeric(Year) + 1, 
                                                 as.numeric(Year)))

espresso<- espresso %>% mutate(Week_Adj = ifelse(Year_Adj == 2019, Week + 52, 
                                                 ifelse(Year_Adj == 2020, Week + 104, Week)))

```

```{r eval=FALSE, include=TRUE}
# Aggregating per week

espresso_agg <- espresso %>% 
                    select(1, 3, 7, 11) %>% 
                    group_by(Store, Week_Adj) %>% 
                    summarise(Units_Sold = sum(Quantity), Avg_Price = mean(Unit_price))
```

```{r eval=FALSE, include=TRUE}
# Importing Salesman Presence Data 
setwd('~/Desktop/RMA/Individual')
presence_2018  <- read_csv('2018.csv') %>% select(-1) 
presence_2019  <- read_csv('2019.csv') %>% select(-1)
```

```{r eval=FALSE, include=TRUE}
# Changing layout of dataframe to have a date column
presence_2018 <- pivot_longer(presence_2018, cols = !Store_ID, names_to = 'Date')
presence_2019 <- pivot_longer(presence_2019, cols = !Store_ID, names_to = 'Date')

# Adding both in one data frame 
presence <- rbind(presence_2018, presence_2019)

names(presence)[3] <- 'Salesperson'
names(presence)[1] <- 'Store'

presence$Store <- as.character(presence$Store)
```
 
```{r eval=FALSE, include=TRUE}
# Aggregating data on a weekly basis as before
presence$Week <-as.numeric(strftime( presence$Date, format = '%V'))
presence$Month <-as.numeric(strftime(presence$Date, format = '%m'))
presence$Year <-as.numeric(strftime(presence$Date, format = '%Y'))

# Adding this here because otherwise the end of a year is labelled as the first week of the year
presence <- presence %>% mutate(Year_Adj = ifelse(Week ==1 & Month == 12, Year + 1, Year))

presence <- presence %>% mutate(Week_Adj = ifelse(Year_Adj == 2019, Week + 52, 
                                                 ifelse(Year_Adj == 2020, Week + 104, Week)))
```

```{r eval=FALSE, include=TRUE}
# Aggregating per week

presence_agg <- presence %>% 
                    select(1, 3, 8) %>% 
                    group_by(Store, Week_Adj) %>% 
                    summarise(Days = sum(Salesperson))
```

```{r eval=FALSE, include=TRUE}
# Merging the two dataframes 
all_data <- inner_join(espresso_agg, presence_agg, by = c('Store', 'Week_Adj'), copy = FALSE)
all_data <- as.data.frame(all_data)
```

```{r eval=FALSE, include=TRUE}
# Replacing Store SAP numbers with Store1, Store2, ... 
stores <- levels(factor(all_data$Store))

new_names <- c()

for (i in 1:length(stores)){
  new_names[i] <- paste('Store', i)
}

all_data$Store <- factor(all_data$Store)
levels(all_data$Store) <- new_names

names(all_data)[2] <- 'Week'
```

```{r eval=FALSE, include=TRUE}
# Exporting as xlsx 
setwd('~/Desktop/RMA/Individual')
write.xlsx(all_data, 'Clean_Data.xlsx') # This is the data set that is imported at the beginning of the analysis
```

### Appendix B (Random Intercepts and Slopes covariance plot)

```{r fig.height=7, fig.width=10}
R1randomeff <- ranef(model2, condVar = TRUE) # To estimate team intercepts and slopes
plot(R1randomeff[[1]], xlab = "Intercept", ylab = "Slope")
abline(h = 0, col = "red")
abline(v = 0, col = "red")
text(R1randomeff$Store[[1]],R1randomeff$Store[[2]],labels=row.names(R1randomeff$Store), cex=0.5,
                    pos=3, offset=0.3)
```

The points in the plot represent the covariance between the intercepts and slopes of the model. The stores at the top-right quadrant of the plot have an above average intercept and slope, thus implying that the number of days has a stronger positive effect. The weekly sales of the stores at the bottom-right quadrant of the plot are above average, but the days effect is below average. This suggests that those stores are performing well despite the presence of a salesperson, and thus the company should not invest a significant amount of their budget to place salespeople in these stores. Finally, several stores are found in the upper-left quadrant, suggesting that despite the lower than average weekly sales, the presence of salespeople had a strong positive impact. Therefore, the marketing manager should prioritize these stores when distributing the number of days among the different stores, as they have more capacity for improvement. If more data was available, a further analysis could be carried out to identify whether the stores in each quadrant are of a particular size category, similar location types etc. 


